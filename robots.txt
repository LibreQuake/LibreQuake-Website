# robots.txt
# https://owly.fans/robots.txt
# Page is under FOPL-ZERO
# https://owly.fans/license/fopl-zero

# Want to know when this page is updated? Follow the RSS!
# https://owly.fans/rss/robots.xml

# This page is also on git:
# Please feel free to suggest a change to this file
# https://github.com/DynTylluan/owly.fans/blob/main/robots.txt (main)
# https://notabug.org/DynTylluan/owly.fans/src/main/robots.txt (mirror)
# https://tildegit.org/cass/owly.fans/src/branch/main/robots.txt (mirror)

# AdIdxBot (Bing/Microsoft)
# Used by Bing Ads for «quality control» for, you guessed it, ads!
# https://bing.com/webmasters/help/which-crawlers-does-bing-use-8c184ec0
User-Agent: AdIdxBot
Disallow: /

# Amazon
# Used for Alexa - does not cite sources.
# https://developer.amazon.com/support/amazonbot
User-Agent: Amazonbot
Disallow: /

# AhrefsBot
# «powerful Web Crawler designed to enhance the functionality of
# the Ahrefs online marketing
# toolset» - not today, thank you!
# https://ahrefs.com/robot
# https://simtechdev.com/blog/good-and-bad-bots-to-control-to-save-server-resources-and-improve-performance
User-Agent: AhrefsBot
Disallow: /

# AI2Bot
# Used to «[Explore] certain domains to find web content. This
# web content is used to train open language models.»
# https://allenai.org/crawler
User-agent: AI2Bot
User-agent: Ai2Bot-Dolma
Disallow: /

# Anthropic
# «AI research and products that put safety at the
# frontier» - no thanks!
# https://anthropic.com
User-agent: anthropic-aianthropic-ai
Disallow: /

# Apple's bot
# Used for Siri and Spotlight Suggestions. Does not cite its sources.
# https://support.apple.com/en-us/HT204683
User-Agent: Applebot
User-agent: Applebot-Extended
User-Agent: AppleNewsBot
Disallow: /

# Awario Crawlers
# Used for «Internet marketers from all over the world» - nope!
# https://awario.com/bots.html
User-agent: AwarioRssBot
User-agent: AwarioSmartBot
Disallow: /

# Barkrowler
# Used for online marketing.
# https://babbar.tech/crawler
User-agent: barkrowler
Disallow: /

# BLEXBot
# «[A]ssists internet marketers» is all I needed to hear.
# http://webmeup-crawler.com
# https://weblynx.pro/bots-you-should-block-to-protect-your-content
User-agent: BLEXBot
Disallow: /

# BuiltWith
# Collects website data and sells it to companies so they can
# know more about your business before they contact you.
# https://builtwith.com/biup
User-agent: BuiltWith
User-agent: BW
User-agent: BW/1.1
Disallow: /

# Bytespider/ByteDance/TikTok bot
# Crawler bot, as operated by ByteDance, the company that owns
# TikTok, is used to scrape up data to train, you guessed
# it, AI/large language model (LLM).
# https://mashable.com/article/tiktok-parent-company-bytedance-web-crawler-25-times-faster-than-openai
# https://servebolt.com/articles/servebolts-decision-to-block-bytespider-and-claudebot
User-agent: AwarioRssBot
User-agent: AwarioSmartBot
Disallow: /

# Claude
# Used to help AI.
# https://claude.ai
User-agent: Claude-Web
User-agent: ClaudeBot
Disallow: /

# Cohere
# Stupid bot used to help artificial intelligence.
# https://cohere.com
User-agent: cohere-ai
Disallow: /

# Common Crawl bot
# From what I've leant from reading online, «[t]he majority of ChatGPT's
# training data comes from the Common Crawl crawler bot».
# Quote: https://datadome.co/threat-research/how-chatgpt-openai-might-use-your-content-now-in-the-future
# https://commoncrawl.org/faq
User-agent: CCBot
Disallow: /

# DataForSeoBot
# «DataForSeoBot is a backlink checker bot that collects SEO
# information from your website and sells it to clients», oh joy.
# https://dataforseo.com/dataforseo-bot
# https://weblynx.pro/bots-you-should-block-to-protect-your-content
User-agent: DataForSeoBot
Disallow: /

# Dataprovider
# Sells data about your site to people.
# https://dataprovider.com/spider
# https://weblynx.pro/bots-you-should-block-to-protect-your-content
User-agent: Dataprovider
Disallow: /

# Diffbot
# «Web Data for your AI» - shut up.
# https://diffbot.com
# https://en.wikipedia.org/wiki/Diffbot
User-agent: Diffbot
Disallow: /

# DotBot
# Yet another data collecting bot that no-one asked for.
# https://simtechdev.com/blog/good-and-bad-bots-to-control-to-save-server-resources-and-improve-performance
User-agent: dotbot
Disallow: /

# EmailSiphon
# Email harvesting bot, get outta here!
User-agent: EmailSiphon
Disallow: /

# Expanse
# Searches the WWW for «malicious URLs» and sells that data on.
# Accoring to the second link, «t is a very aggressive scanner
# that uses a lot of network resources», blocked for that reason.
# https://live.paloaltonetworks.com/t5/general-topics/spurious-hits-from-the-expanse-webcrawler/td-p/447239
# https://weblynx.pro/bots-you-should-block-to-protect-your-content
User-agent: Expanse
Disallow: /

# FacebookBot
# Used to «improve language models for our speech recognition technology»,
# so more AI rubbish that I don't want from a company that I don't like.
User-Agent: FacebookBot
User-Agent: facebookexternalhit
User-agent: Meta-ExternalAgent
User-agent: Meta-ExternalFetcher
Disallow: /

# FriendlyCrawler
# Not sure if this one has its own website, so finding out
# information on this bot was kind of hard, according to the
# link below, it runs on Amazon's autonomous systems (AWS) and
# is used for «machine learning experiments».
#
# The bot also does not respect robots.txt (great), so
# ultimately putting it on this page is pointless, but as
# a sysop, you should still be aware of it.
# https://weblynx.pro/bots-you-should-block-to-protect-your-content
User-agent: FriendlyCrawler
Disallow: /

# go-http-client
# Quoting the second link «often seen as the user agent of bots
# probing for vulnerabilities. Any legitimate human users will
# not use this user agent string. Though the tool can be used
# for legitimate purposes, it is mainly used by malicious bots».
#
# While it's not the name for the bot, I opted to also block
# «go-https-client» (with an «s»).
# https://github.com/bozd4g/go-http-client
# https://weblynx.pro/bots-you-should-block-to-protect-your-content 
User-agent: go-http-client
User-agent: go-https-client
Disallow: /

# Google's AdSense/StoreBot bots
# Go away - I don't want you here.
# https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers
User-Agent: AdsBot-Google-Mobile
User-Agent: Storebot-Google
User-Agent: AdsBot-Google
User-agent: Mediapartners-Google
Disallow: /

# Google's AI training bot
# I don't want to have my content used without giving me credit.
# https://blog.google/technology/ai/an-update-on-web-publisher-controls
User-agent: Google-Extended
Disallow: /

# GPTBot is OpenAI's web crawler
# I do not want it to use my content as it does not cite its sources.
# https://platform.openai.com/docs/gptbot
User-agent: GPTBot
User-agent: ChatGPT-User
Disallow: /

# ia_archiver
# In the past, the Internet Archive (https://archive.org) used to have a
# bot called ia_archiver that you could allow (or disallow) to index your
# website, sometimes around 2017, this crawler stopped obeying robots.txt.

# Putting ia_archiver and setting it to «Disallow: /» will not work. If
# you really want your website to not be seen on this archive, you will
# need to E-Mail info@archive.org.
# https://blog.archive.org/2017/04/17/robots-txt-meant-for-search-engines-dont-work-well-for-web-archives
# https://web.archive.org/web/20150322111536/http://archive.org/about/exclude.php
# https://blog.reputationx.com/block-wayback-machine

# Magpie Crawler
# Used to download pages and get «indexed and analysed by our system».
# I don't want to use your service!
User-agent: magpie-crawler
Disallow: /

# MauiBot
# «An unidentified bot [...] usually being blocked to avoid an
# enormous volume of requests it does»
# https://simtechdev.com/blog/good-and-bad-bots-to-control-to-save-server-resources-and-improve-performance

# MindSpider
# According to cdc.gov, this bot «appears to be ill-behaved», so it is
# being blocked here.
# https://www.cdc.gov/robots.txt
User-agent: MindSpider
Disallow: /

# Pinterest crawler
# I don't want my website to be used to «collect rich metadata like the
# price, description and availability of your products» or however they
# want to word that rubbish.
# https://help.pinterest.com/en/business/article/pinterest-crawler
User-agent: Pinterestbot
Disallow: /

# SEMrushBot
# «SEMrushBot is the search SEO bot software that SEMrush sends out to
# discover and collect new and updated web data. Data collected by
# SEMrushBot is used in the reports researches and graphs.»
# This sounds terrible.
# https://simtechdev.com/blog/good-and-bad-bots-to-control-to-save-server-resources-and-improve-performance
User-agent: SemrushBot
User-agent: SemrushBot-BA
User-agent: SemrushBot-BM
User-agent: SemrushBot-CT
User-agent: SemrushBot-SA
User-agent: SemrushBot-SI
User-agent: SemrushBot-SWA
Disallow: /

# This is where I copy Bytemoth
# I am taking it in good faith that Bytemoth knows what he's doing and that
# the following bots are for marketers - so I am blocking them.
# As taken from the following URL: http://bytemoth.nfshost.com/robots.txt
User-agent: adsbot
User-agent: Pandalytics
User-agent: serpstatbot
User agent: MTRobot
User-agent: PageThing
Disallow: /


# Bots that you might like to block
# There are a few bots that I am okay/don't really care all that much if
# they index my website, but not everyone might be okay with them, so
# if you copy this TXT on your website and want to have the bot not index
# it, simply remove the «#» before the «User-agent» and «Disallow» part.

# DuckDuckGo
# The search engine website uses the following bots to index sites.
# https://duckduckgo.com/duckduckbot
# https://duckduckgo.com/duckduckgo-help-pages/results/duckduckbot
# https://duckduckgo.com/duckduckgo-help-pages/results/sources
# User-agent: DuckDuckBot
# Disallow: /

# Other Google bots
# Don't like Google? Unmark these.
# https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers
# User-agent: AdsBot-Google
# User-agent: AdsBot-Google-Mobile
# User-agent: APIs-Google
# User-agent: FeedFetcher-Google
# User-agent: Google-Extended
# User-agent: Google-InspectionTool
# User-agent: google-speakr
# User-agent: Googlebot
# User-agent: Googlebot-Image
# User-agent: Googlebot-News
# User-agent: Googlebot-Video
# User-agent: GoogleOther
# User-agent: GoogleOther-Image
# User-agent: GoogleOther-Video
# User-agent: GoogleProducer
# User-agent: GoogleProducer; (+http://goo.gl/7y4SX)
# User-agent: Mediapartners-Google
# User-agent: Storebot-Google
# Disallow: /

# Don't allow any bots
# If you don't want *any* good bots browsing your website, then unmark
# this. Please note that there are some bots that still don't obay
# robots.txt at all, like ia_archiver.
# User-agent: *
# Disallow: /

# Allow all bots
# If you do, however, want to allow all bots, you should uncheck the
# following. You can also just *not* have a robots.txt and it'll
# work all the same.
# User-agent: *
# Allow: /